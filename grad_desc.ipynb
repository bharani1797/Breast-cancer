{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "grad_desc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharani1797/ML_Assignment/blob/master/grad_desc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "I8uBh7sv2bxk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0c520668-bb7e-4175-fe3c-4603e4ba5b7a"
      },
      "cell_type": "code",
      "source": [
        "from numpy import *\n",
        "import io\n",
        "\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['data.csv']))\n",
        "\n",
        "# minimize the \"sum of squared errors\". This is how we calculate and correct our error\n",
        "def compute_error_for_line_given_points(b,m,points):\n",
        "\ttotalError = 0 \t#sum of square error formula\n",
        "\tfor i in range (0, len(points)):\n",
        "\t\tx = points[i, 0]\n",
        "\t\ty = points[i, 1]\n",
        "\t\ttotalError += (y-(m*x + b)) ** 2\n",
        "\treturn totalError/ float(len(points))\n",
        "\n",
        "def step_gradient(b_current, m_current, points, learning_rate):\n",
        "\t#gradient descent\n",
        "\tb_gradient = 0\n",
        "\tm_gradient = 0\n",
        "\tN = float(len(points))\n",
        "\tfor i in range(0, len(points)):\n",
        "\t\tx = points[i, 0]\n",
        "\t\ty = points[i, 1]\n",
        "\t\tb_gradient += -(2/N) * (y - (m_current * x + b_current))\n",
        "\t\tm_gradient += -(2/N) * x * (y - (m_current * x + b_current))\n",
        "\tnew_b = b_current - (learning_rate * b_gradient)\n",
        "\tnew_m = m_current - (learning_rate * m_gradient) \n",
        "\treturn [new_b,new_m]\n",
        "\n",
        "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iteartions):\n",
        "\tb = starting_b\n",
        "\tm = starting_m\n",
        "\tfor i in range(num_iteartions):\n",
        "\t\tb,m = step_gradient(b, m, array(points), learning_rate)\n",
        "\treturn [b,m]\n",
        "\n",
        "def run():\n",
        "\t#Step 1: Collect the data\n",
        "\tpoints = genfromtxt('data.csv', delimiter=',')\n",
        "\t#Step 2: Define our Hyperparameters\n",
        "\tlearning_rate = 0.0001 #how fast the data converge\n",
        "\t#y=mx+b (Slope formule)\n",
        "\tinitial_b = 0 # initial y-intercept guess\n",
        "\tinitial_m = 0 # initial slope guess\n",
        "\tnum_iteartions = 1000\n",
        "\tprint(\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
        "  print(\"Running...\")\n",
        "  [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
        "  print(\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)))\n",
        "\n",
        "# main function\n",
        "if __name__ == \"__main__\":\n",
        "  run()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-72baed7875a9>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    print(\"Running...\")\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    }
  ]
}